The Transformer uses multi-head attention in three different ways: